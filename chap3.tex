\chapter{Existing Work on API Summarisation}
\label{chap:existing-work}

\section{Defining the Problem}
\label{sec:problem-definition}

As mentioned in \Cref{sec:hypothesis}, the problem we are facing in general is that of mining API usage patterns, which has been clearly described by \nolink{\citeauthor{Ishag:2016}} in \cite{Ishag:2016}. However, here we specifically define four main features that a system that performs API usage mining should exhibit. That is, given a -local- repository of Java source code files that are relevant to the target API, the system to be developed should be able to:

\begin{itemize}
\setlength\itemsep{0.1em}
\item[\featureyes] Consider the structure of the source code files
\item[\featureyes] Cluster similar usage examples
\item[\featureyes] Summarise the examples into concise and readable snippets
\item[\featureyes] Present a ranked list of snippets to the users
\end{itemize}

As revealed from the above features, our primary goal is to mine common usage snippets that are concise and readable, instead of mining and presenting frequent sequences of API method calls. In addition to that, we are going to explore ML and NLP techniques in order to approach the problem efficiently, many of which have not been applied to any systems before.

The most striking examples of similar systems that conduct API usage mining are analysed in the next two sections. We divide them into two categories; the first one consists of systems that present sequences, and the second one of these that present snippets to the users.


\section{Systems that Output Sequences}
\label{sec:sequence-systems}

One of the very first systems to mine API usage patterns has been the \textit{MAPO} system \citep{Xie:2006}. Its objective has been to generate usage patterns, that could then act as an index for the recommendation of code snippets. MAPO is an ordinary example of a system that performs frequent sequence mining. As regards the representation of the source code, the system firstly extracts API method call sequences from the given source code files. After that, MAPO clusters these sequences, on its most recent version \citep{Zhong:2009}, with the aim of improving the quality of the mined patterns. On its next step, the system combines the mined frequent call sequences -extracted using the \textit{SPAM} sequence miner \citep{Ayres:2002}- from each cluster, to produce patterns that are eventually presented to the users. The latest version of the MAPO system, accompanies the API sequences with their associated snippets. However, it is still more of a sequence-based approach, as it shows the source code of the client method without proceeding to any summarisation, while it also avoids to consider the structure of the snippets.

In \citep{Wang:2013} the authors argue that MAPO outputs a large number of usage patterns, many of which are moreover redundant. To overcome these issues, \nolink{\citeauthor{Wang:2013}} define \textit{scalability}, \textit{succinctness} and \textit{high-coverage} as the main characteristics by which an API miner should abide. Based on these characteristics, their \textit{UP-Miner} system focuses on qualitative usage patterns. Their approach includes a two-step clustering, as well as mining of frequent closed sequences -using the \textit{BIDE} algorithm \citep{Wang:2004}- in order to mine usage patterns of single API methods. Although the UP-Miner tool is able to extract even more rear patterns, which cannot be extracted by the MAPO system, we believe that the use of probabilistic graphs of API method calls, in order for the first system to present the results may confuse the users, who definitely prefer simpler ways for the presentation of such results (e.g. ranked lists).

A brand new system that introduces several novelties with the purpose of presenting API usage patterns, in the form of sequences, is presented in \citep{Fowkes2:2015}. \nolink{\citeauthor{Fowkes2:2015}} point out that most of the heavily used frequent sequence mining methods require multiple user-defined parameters, that need to be hard-coded, and which are additionally hard to tune. With the aim of avoiding manually given parameters, they present a near parameter-free probabilistic algorithm (\textit{PAM}), which is able to cluster method sequences, and to extract ``the most informative API call patterns''. In this way, \nolink{\citeauthor{Fowkes2:2015}} apply techniques used in statistics and in the ML field to source code. This is the first system to mine API calls at GitHub scale, and to evaluate using handwritten examples that exist in libraries' \texttt{examples} directory, as \textit{gold standards}\footnote{A \textit{gold standard} (also known as ``oracle example'') is supposed to be an ideal example, based on which the quality of other examples can be evaluated.}. Using examples written by the developers of a library to evaluate API miners seems a promising concept, that would enable the drawing of more objective conclusions, compared to the ones drawn from user studies. Additionally, using the libraries' \texttt{examples} directory allows the use of automatic evaluation methods, which would not usually be possible. We will dive more into this concept in \Cref{chap:evaluation}. Although the outcome of this system is a list of method sequences rather than of usage snippets, we are going to leverage techniques used in this paper.


\section{Systems that Output Snippets}
\label{sec:snippet-systems}

Instead of presenting API-relevant sequences to the users, many systems during the last years output snippets. A possible usage scenario of such systems includes the enrichment of the APIs' documentation.

Upon improving the documentation of a target API, \nolink{\citeauthor{Kim:2009}} \cite{Kim:2009, Îšim:2010, Kim:2013} firstly exploit slicing techniques, in order to summarise snippets retrieved by the \textit{Koders}\footnote{Renamed to \textit{Black Duck Open Hub Code Search}, before being discontinued in June 2016.} CSE. Their summarisation algorithm preserves the lines of the source code that are relevant to the target API, using common backward and forward slicing. The next step is to represent the summarised snippets using feature vectors. For this task the system makes use of the \textit{DECKARD} clone detection algorithm \cite{Jiang:2007}, which is a tree-based similarity detection algorithm, that proposes vectors for approximating the semantic context of ASTs. \nolink{\citeauthor{Kim:2009}} feed this algorithm with Java source code elements, while they also include additional features, such as the frequency of the query API method, and the lines of code in the snippets. Having extracted the feature vectors, they then experiment with three different approaches, in order to organise the generated examples; the \textit{eXoaCluster} algorithm, that uses the $k$-means clustering technique to cluster similar usage examples, the \textit{eXoaRank} algorithm, which ranks the examples based on their probability/frequency, and the \textit{eXoaHybrid} algorithm, that combines both of the aforementioned approaches. A disadvantage of the \textit{eXoaDocs} system we identify is that the system is not really capable of mining frequent patterns that include multiple API method calls. This is based on the fact that their feature vectors do not include any such information. Thus, the system mainly targets usage examples of single API methods.

Another approach that is not quite common is that followed in \cite{Wang:2011}, where \nolink{\citeauthor{Wang:2011}} make use of the \textit{Google Search} web search engine, in order to find useful examples. The \textit{APIExample} tool extracts code snippets, as well as their surrounding text from webpages, with the purpose of forming usage examples. It then clusters\footnote{Here, only client methods that contain the same API calls are clustered together.} these examples, and ranks them using intra-cluster and inter-cluster ranking heuristics. However, this tool does not summarise the usage examples, while we could argue that the clustering technique used does not lead to significant reduction in the number of the collected examples.

A more recent system that is presented in \cite{Montandon:2013} is the \textit{APIMiner}\footnote{\url{http://apiminer.org/}}. One of the highlights of the first version of this system is the summarisation algorithm that has been implemented by the authors, which uses backward and forward slicing, in order to preserve only the API-relevant statements of a source code file. In its most recent version, which is based on the work presented in \cite{Borges:2014}, the APIMiner leverages association rule techniques, and uses an improved version of the summarisation algorithm, with the aim of resolving variable types, or adding abstractive comments. However, it does not cluster similar usage examples, and additionally, our investigation of the system, revealed that most of the examples show the usage of a single API method.

Even when slicing is employed in the aforementioned systems, mined examples often contain extraneous statements, as pointed out by \nolink{\citeauthor{Buse:2012}} \cite{Buse:2012}. Therefore, the authors introduce a novel system that synthesises representative and well-typed usage examples. To the best of our knowledge, this in the first system to synthesise abstract examples, which it then presents to the users. This system combines \textit{path sensitive data flow analysis}\footnote{According to \nolink{\citeauthor{Winter:2013}} \cite{Winter:2013}, in path sensitive data-flow analysis (or path-sensitive DFA), path information that reveals whether a path is feasible or not is collected, with the aim to report bugs from feasible paths only.}, clustering, and pattern abstraction, in order to present usage examples that are quite complete and abstract. The mined snippets include abstract naming, as well as helpful code, such as \texttt{try} and \texttt{catch} statements. However, the fact that the source code is represented using graphs makes the system really complex, and probably inefficient as pointed out in \citep{Wang:2013}.